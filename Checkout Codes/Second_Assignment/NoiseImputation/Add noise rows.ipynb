{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python code to generate \n",
    "# random numbers and \n",
    "# append them to a list \n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "  \n",
    "# Function to generate \n",
    "# and append them  \n",
    "# start = starting range, \n",
    "# end = ending range \n",
    "# num = number of  \n",
    "# elements needs to be appended \n",
    "#random_val\n",
    "def RandNoise(end, start, num): \n",
    "    res = []\n",
    "    \n",
    "    for j in range(num): \n",
    "        res.append(round(random.uniform(start, end),3)) \n",
    "  \n",
    "    return res \n",
    "  \n",
    "def getNoiseData(dataSet,percent):\n",
    "#     dataSet = np.delete(dataSet,(0), axis=0)\n",
    "    size = dataSet.shape\n",
    "    rows = size[0]\n",
    "    #print(rows)\n",
    "    cols = size[1]-1\n",
    "   \n",
    "    num = (int)(rows*(percent/100))\n",
    "    end = np.amax(dataSet,axis=0)\n",
    "    start = np.amin(dataSet,axis=0)\n",
    "    \n",
    "    #print(start)\n",
    "    #print(end)\n",
    "    i = 0;\n",
    "    random_val = []\n",
    "    while i<cols:\n",
    "        random_val.append(RandNoise(start[i],end[i], num))\n",
    "        #print(start[i])\n",
    "        #print(end[i])\n",
    "        i = i+1\n",
    "    return random_val\n",
    "#print(random_rows)\n",
    "\n",
    "\n",
    "def Rand(start, end, num): \n",
    "    res = [] \n",
    "  \n",
    "    for j in range(num): \n",
    "        res.append(random.randint(start, end)) \n",
    "  \n",
    "    return res\n",
    "  \n",
    "def getNoisePlaces(dataSet,percent):\n",
    "    size = dataSet.shape\n",
    "    rows = size[0]\n",
    "    num = (int)(rows*(percent/100))\n",
    "    start = 0\n",
    "    end = rows-1\n",
    "    random_rows=Rand(start, end, num)\n",
    "    return random_rows\n",
    "\n",
    "#read both csv files to np array\n",
    "\n",
    "orginal_dataset = pd.read_csv('../NumericData/clean.csv')\n",
    "copy_dataset=orginal_dataset\n",
    "\n",
    "percent = 20\n",
    "random_data = getNoiseData(copy_dataset,percent)\n",
    "\n",
    "size = copy_dataset.shape\n",
    "rows = size[0]\n",
    "cols = size[1]-1\n",
    "\n",
    "print(size)\n",
    "k = 0\n",
    "while k<cols:\n",
    "    random_places = getNoisePlaces(copy_dataset,percent)\n",
    "    #print(len(random_places))\n",
    "    j = 0\n",
    "    while j<len(random_places):\n",
    "        a = random_data[k][j]\n",
    "        row2change = random_places[j]\n",
    "        #print(row2change)\n",
    "        copy_dataset.iloc[row2change,k] = a\n",
    "        \n",
    "        #copy_dataset.itemset((j,k),a)\n",
    "        \n",
    "        #print(copy_dataset[j,k])\n",
    "        #print(random_data[k][j])\n",
    "        j = j+1\n",
    "    k = k+1\n",
    "\n",
    "    \n",
    "data = np.asarray(copy_dataset)\n",
    "np.savetxt(\"../NumericData/foo.csv\", data, delimiter=\",\")\n",
    "df = pd.DataFrame(copy_dataset)\n",
    "df.to_csv(\"../NumericData/noisy.csv\")\n",
    "\n",
    "copy_dataset.shape\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "new_noise_data = noise_dataset.iloc[:,0:8]\n",
    "prediction = orginal_dataset.iloc[:,8:]\n",
    "new_noise_data = np.append(new_noise_data,prediction,axis=1)\n",
    "print(new_noise_data.shape)\n",
    "print(prediction.shape)\n",
    "#print(noise_dataset)\n",
    "\n",
    "#replace data in np array\n",
    "\n",
    "for pos in random_rows:\n",
    "    #print(pos)\n",
    "    currdata = new_noise_data[pos:pos+1,:]\n",
    "    positivedata=np.absolute(currdata)\n",
    "    #print(currdata)\n",
    "    copy_dataset[pos:pos+1] = positivedata\n",
    "    \n",
    "copy_dataset=np.absolute(copy_dataset)\n",
    "#print(copy_dataset.shape)\n",
    "#copy_dataset = copy_dataset.iloc[:,1:]\n",
    "#print(copy_dataset.shape)\n",
    "df = pd.DataFrame(copy_dataset)\n",
    "df.to_csv(\"C:/Users/souro/Desktop/MachineLearningProject/Datasets/pima-indians-diabetes-database/pima_indian_25_per.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
